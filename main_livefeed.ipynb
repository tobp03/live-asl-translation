{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751d809a",
   "metadata": {},
   "source": [
    "# Full Pipeline for Live Sign Language Translation with Citizen ASL Dataset\n",
    "\n",
    "This notebook implements a complete pipeline for converting sign language video into live text predictions using pose estimation and a language model.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Landmark Extraction**: Extract body and hand landmarks from Citizen ASL sign videos using [MediaPipe Holistic](https://developers.google.com/mediapipe).\n",
    "2. **Preprocessing** : Normalize and split the landmark sequences into training, validation, and test sets.\n",
    "3. **Model Training** : Train a deep learning model on the preprocessed data.\n",
    "4. **Live translation** : Use a trained model and LLM in a real-time translation system.\n",
    "\n",
    "## Requirements:\n",
    "- Install dependencies via:\n",
    "    ```bash\n",
    "    pip install -r requirements.txt \n",
    "    ```\n",
    "    **Or**, install them manually. Make sure you are using the following versions:\n",
    "    - Python: 3.10.17\n",
    "    - TensorFlow: 2.19.0\n",
    "    - MediaPipe 0.10.9\n",
    "    - OpenAI 1.82.0\n",
    "\n",
    "- Obtain an API key for a Large Language Model (LLM):\n",
    "    - This pipeline uses [OpenAI's GPT API](https://platform.openai.com/) for language enhancement in the live translation phase.\n",
    "    - Note: OpenAI is not free. You must have a valid and funded API key.\n",
    "\n",
    "- **Landmark extraction** requires [ASL Citizen Dataset](https://www.microsoft.com/en-us/research/project/asl-citizen/) downloaded\n",
    "\n",
    "> **Preprocessed data** and **model training** is available on [this notebook](https://www.kaggle.com/code/tobypu/aslcitizen-top200-training).\n",
    "\n",
    "> **Live translation** is available for testing using a trained model covering 200 unique glosses (271 total classes including duplicates). To use it, specify the path to the trained model and the accompanying index_to_glos_200.json file, which maps model outputs to gloss labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b78644",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 15:16:12.758524: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-08 15:16:12.761459: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-08 15:16:12.769440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749370572.782765   48545 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749370572.786568   48545 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749370572.797503   48545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749370572.797521   48545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749370572.797523   48545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749370572.797524   48545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-08 15:16:12.801566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.17 (main, Jun  8 2025, 14:44:46) [GCC 15.1.1 20250521 (Red Hat 15.1.1-2)]\n",
      "TensorFlow: 2.19.0\n",
      "MediaPipe: 0.10.9\n",
      "OpenAI: 1.82.0\n",
      "cv2: 4.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import openai\n",
    "import cv2\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"MediaPipe:\", mp.__version__)\n",
    "print(\"OpenAI:\", openai.__version__)\n",
    "print(\"cv2:\", cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21733526",
   "metadata": {},
   "source": [
    "The following modules are imported for different stages of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578393c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import asl_citizen_MP_encoding\n",
    "from utils import preprocessing_split \n",
    "from utils import train_model \n",
    "from utils import live_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62bd63",
   "metadata": {},
   "source": [
    "## 1. Landmark Extraction\n",
    "This step converts raw sign videos from the Citizen ASL dataset into landmark sequences using MediaPipe Holistic.\n",
    "\n",
    "### Input\n",
    "- Sign videos from the Citizen ASL dataset\n",
    "- Split CSV files: `train.csv`, `test.csv`, and `val.csv` under `splits/` directory\n",
    "- Citizen ASL dataset folders and videos should not be modified or renamed from their original structure.\n",
    "\n",
    "### What It Does\n",
    "For each video, the pipeline:\n",
    "1. Extracts 3D landmarks for:\n",
    "    - 33 pose landmarks\n",
    "    - 21 left-hand landmarks\n",
    "    - 21 right-hand landmarks\n",
    "    - A subset of 32 facial landmarks\n",
    "2. Combines them into `(frames, 107, 3)` NumPy array\n",
    "3. Stores only the`(x,y)` coordinates and `gloss` to a `.npz` file.\n",
    "### Output\n",
    "- One `.npz` file per video saved under `processed_save_dir`\n",
    "- Each `.npz` file contains:\n",
    "    - `landmarks`: `np.darray` of shape (frames,107,2)\n",
    "    - `gloss`: The sign label\n",
    "\n",
    "\n",
    "> **Note** : Approximately takes about 3-4 days to process all videos with Mediapipe Holistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed88aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir='/ASL_Citizen/videos'\n",
    "processed_save_dir='processed_all'\n",
    "asl_citizen_MP_encoding.process_asl_dataset(video_dir,processed_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dea71f",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "This step prepared the extracted landmarks for model training by normalizing, pad the data, and encode the labels.\n",
    "\n",
    "### What It Does\n",
    "1. Generate Label and One-Hot Encoders\n",
    "    - Reads the glosses from `train.csv`, `val.csv`, or `test.csv`\n",
    "    - Creates:\n",
    "        - A label encoder (gloss -> integer)\n",
    "        - A one-hot encoder (integer -> one-hot vector)\n",
    "        - A gloss_to_index.json dictionary for mapping (gloss -> integer)\n",
    "2. Encode Labels\n",
    "    - For each `.npz` file:\n",
    "        - Loads the gloss label\n",
    "        - Converts it to one-hot format using the encoders\n",
    "    - Saves the resulting arrays as `y_train.npy`, `y_val.npy`, and `y_test.npy` under `save_dir`\n",
    "3. Preprocess Landmark Features\n",
    "    - Loads landmark arrays of shape `(frames, 107, 2)`\n",
    "    - Removes:\n",
    "        - Pose landmarks 0-10 and 23-32\n",
    "    - Pads or repeats frame to a fixed length of 150\n",
    "    - Applies normalization:\n",
    "        - Anchor-based normalization for body, face, and arm landmarks.\n",
    "        - Hand normalization to bound hand keypoints in `[-0.5,0.5]`, centered at `(0,0)`\n",
    "    - Saves the processed data as `x_train.npy`, `x_val.npy`, and `x_test.npy` in shape `(videos, 160,86 , 2)` under `save_dir`\n",
    "\n",
    "### Input\n",
    "- `.npz` files from the landmark extrtaction step (under `save_dir_name/{split}`)\n",
    "- Corresponding `train.csv`, `val.csv`, or `test.csv` file\n",
    "\n",
    "### Output\n",
    "- One-hot encoded label arrays: `y_train.npy`, `y_val.npy`, and `y_test.npy`\n",
    "- Preprocessed landmark arrays: `x_train.npy`, `x_val.npy`, and `x_test.npy`\n",
    "\n",
    "> Note: This step applies pose normalization, arm alignment, and hand bounding box to remove spatial and body proportionality bias. For a detailed explanation and justification of these choices, please refer  to [our research paper](https://doi.org/10.26877/sj5scb03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7aba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Generate Label Encoder, OneHot Encoder, and Gloss Dictionary ===\n",
    "csv_path='ASL_Citizen/splits/test.csv' #location of test/train/val.csv\n",
    "save_dict_path='gloss_to_index' #Path and dictionary name\n",
    "label_encoder, onehot_encoder, gloss_to_index = preprocessing_split.generate_gloss_dictionary(csv_path,save_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626cffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2731"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gloss_to_index) #STRING (GlOSS) TO UNIQUE INTEGER (0-2730)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfdbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'model_train_data'\n",
    "processed_save_dir='/processed_all'\n",
    "\n",
    "#Create y_train, y_test, and y_val\n",
    "preprocessing_split.encode_labels(label_encoder, onehot_encoder, processed_save_dir, save_dir)\n",
    "\n",
    "#Create X_train, X_test, and X_val\n",
    "preprocessing_split.preprocess_and_save_x(processed_save_dir, save_dir, split='train')\n",
    "preprocessing_split.preprocess_and_save_x(processed_save_dir, save_dir, split='test')\n",
    "preprocessing_split.preprocess_and_save_x(processed_save_dir, save_dir, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d70fe",
   "metadata": {},
   "source": [
    "### 3. Model Training\n",
    "\n",
    "This section trains a GRU-based model using the preprocessed sign language data. The training logic is defined in `train_model.py`, which consists of two main components:\n",
    "\n",
    "#### A. `data_loader(...)`\n",
    "This function loads and optionally filters the training, validation, and test datasets:\n",
    "- Loads `.npy` files containing pose features and one-hot encoded labels.\n",
    "- Due to prediction results, labels are converted back from one-hot encoded vectors into unique integer class labels.\n",
    "- Supports partial gloss filtering by mapping similar gloss variants (ex: `GO1` and `GO2` -> `GO`). \n",
    "- Optionally merges the test set into training for cases like leaderboard training.\n",
    "- Returns: tuples of `(X_train, y_train)`, `(X_val, y_val)`, `(X_test, y_test)`, and a decoder dictionary (if filtered) (decoder maps integer -> gloss).\n",
    "\n",
    "### B. `train_gru_model(...)`\n",
    "Trains a sequential GRU-based neural network using TensorFlow/Keras:\n",
    "- Two GRU layers (`386` units and followed by `192` units).\n",
    "- Dropout layers for regularization.\n",
    "- Final `Dense` layer with `softmax` for multi-class classification.\n",
    "- Automatically saves the **best-performing model** on validation accuracy.\n",
    "- If test data is passed, accuracy and macro F1 score are computed.\n",
    "- The final model is saved as `best_model.keras`.\n",
    "\n",
    "---\n",
    "\n",
    "Training was performed on **Kaggle** using GPU acceleartion. The notebook is available [here](https://www.kaggle.com/code/tobypu/aslcitizen-top200-training)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83b5e7",
   "metadata": {},
   "source": [
    "We selected the **top 200 most commonly used signs** based on [HandSpeak's list of most-used signs](https://www.handspeak.com/word/most-used/). These signs are listed in `.txt`file, each separtated by a newline. \n",
    "\n",
    "We also moved test data into train for better interference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a511e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (val_X, val_y), (_, _), decoder = train_model.data_loader(\n",
    "    data_dir=\"/kaggle/input/citizen-asl-mediapipe-encoded-and-preprocessed\",\n",
    "    gloss_to_index_dir='/kaggle/input/citizen-asl-mediapipe-encoded-and-preprocessed/gloss_to_index.json',\n",
    "    filtered_txt_path=\"/kaggle/input/top200citizen/Citizen200.txt\",\n",
    "    merge_test_to_train=True\n",
    ")\n",
    "\n",
    "#Saves the mapping\n",
    "with open(\"index_to_gloss_200.json\", \"w\") as f:\n",
    "    json.dump(decoder, f, indent=2)\n",
    "\n",
    "\n",
    "model = train_model.train_gru_model(\n",
    "    train_X, train_y,\n",
    "    val_X, val_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111abb35",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ec64a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">386</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">648,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">386</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">334,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">271</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">52,303</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m386\u001b[0m)       │       \u001b[38;5;34m648,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m386\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │       \u001b[38;5;34m334,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m271\u001b[0m)            │        \u001b[38;5;34m52,303\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,104,591</span> (11.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,104,591\u001b[0m (11.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,034,863</span> (3.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,034,863\u001b[0m (3.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,069,728</span> (7.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,069,728\u001b[0m (7.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model=load_model('models/best_model200.keras')\n",
    "# Show a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e5b16",
   "metadata": {},
   "source": [
    "### 4. Live translation\n",
    "\n",
    "The `start_live_feed()` function is responsible for capturing webcam input, performing real-time sign language recognition using a trained GRU model, and generating meaningful translations through a language model (OpenAI GPT).\n",
    "\n",
    "#### Input Arguments\n",
    "   - `model_path`: Path to the `.keras` model file containing the trained GRU network for recognizing sequences of body/keypoint features.\n",
    "   - `encoder_path`: JSON file mapping predicted class indices to sign language glosses (e.g., `{0: \"HELLO\", 1: \"THANK-YOU\", ...}`).\n",
    "   - `client`: Load OpenAI API key stored in `.env`\n",
    "   - `threshold` : Controls the sensitivity to motion. A lower value makes the system more sensitive, triggering capture with smaller movements. A higher value requires more movement to start capturing, making it less sensitive. Default is `1.2` (float)\n",
    "   - `webcam` : Specifies which webcam to use webcam device. Default is `0`. If you have multiple, try `1`, `2`, etc (integer)\n",
    "   - `complexity_setting` : Sets the model complexity for the MediaPipe Holistic pipeline. (`0` : Fastest but least accurate, `1` : Balanced, `2` : Most accurate but slowest). Default is `0`\n",
    "\n",
    "#### **Flowchart Summary**\n",
    "![Flowchart](flowchart.jpg)\n",
    "\n",
    "#### **Creating `.env` API KEY file**\n",
    "1. Open a text editor and add your OpenAI API key like this:\n",
    "    ```bash\n",
    "    API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "    ```\n",
    "2. Save the file as `.env` in your project repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33410f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 15:17:33.830891: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749370654.052871   48545 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1749370654.057886   48859 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.6), renderer: AMD Radeon Graphics (radeonsi, renoir, ACO, DRM 3.61, 6.14.9-300.fc42.x86_64)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n"
     ]
    }
   ],
   "source": [
    "client = live_translation.get_client('.env') #Make sure to have API key ready\n",
    "\n",
    "live_translation.start_live_feed(model_path='models/best_model200.keras',\n",
    "                encoder_path='models/index_to_gloss_200.json',\n",
    "                client=client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
